# -*- coding: utf-8 -*-
"""Desafio04_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U37xcrtFoiRoaHr6NA4LAK-kskHxXHHE
"""

class ZipDownloaderConfig:
    def __init__(self):
        self._zip_url = 'https://drive.google.com/uc?id=1SzLGxIRo2j8eHPdqe06xXxtoGGrWHoJ5'
        self._zip_output_path = '/content/planilhas.zip'
        self._extract_folder_path = '/content/planilhas'

    def get_zip_url(self):
        return self._zip_url

    def get_zip_output_path(self):
        return self._zip_output_path

    def get_extract_folder_path(self):
        return self._extract_folder_path

from google.colab import userdata

class DatabaseConfig:
    def __init__(self):
        self._db_host = userdata.get('SUPABASE_HOST')
        self._db_name = userdata.get('SUPABASE_NAME')
        self._db_user = userdata.get('SUPABASE_USER')
        self._db_pass = userdata.get('SUPABASE_PASS')
        self._db_port = userdata.get('SUPABASE_PORT')

    def get_db_host(self):
        return self._db_host

    def get_db_name(self):
        return self._db_name

    def get_db_user(self):
        return self._db_user

    def get_db_pass(self):
        return self._db_pass

    def get_db_port(self):
        return self._db_port

import os
import psycopg2
import pandas as pd
import re
import unicodedata
from google.colab import userdata

class DBUtils:
    def __init__(self):
        self.db_config = DatabaseConfig()

    def table_exists(self, table_name: str) -> bool:
        table_name = table_name.lower()
        conn = psycopg2.connect(
            host=self.db_config.get_db_host(),
            dbname=self.db_config.get_db_name(),
            user=self.db_config.get_db_user(),
            password=self.db_config.get_db_pass(),
            port=self.db_config.get_db_port()
        )
        cur = conn.cursor()
        cur.execute("""
            SELECT EXISTS (
                SELECT 1
                FROM information_schema.tables
                WHERE table_schema = 'public'
                AND table_name = %s
            );
        """, (table_name,))
        exists = cur.fetchone()[0]
        cur.close()
        conn.close()
        return exists

    def drop_table(self, table_name: str):
        """
        Generates and executes a DROP TABLE script for a given table name in PostgreSQL.
        """
        table_name_lower = table_name.lower()
        sql_script = f"DROP TABLE IF EXISTS public.{table_name_lower};"
        print(f"\nGenerated SQL Script to drop table:")
        print(sql_script)

        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()
            cur.execute(sql_script)
            conn.commit()
            cur.close()
            conn.close()
            print(f"Table '{table_name_lower}' dropped successfully if it existed.")
        except Exception as e:
            print(f"Error dropping table '{table_name_lower}': {e}")

    def execute_query(self, sql_query: str):
        """
        Executes a given SQL query against the PostgreSQL database and returns the results.
        """
        print("\nExecuting SQL query...")
        print(sql_query)
        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()
            cur.execute(sql_query)
            # If it's a SELECT query, fetch results
            if sql_query.strip().upper().startswith('SELECT'):
                results = cur.fetchall()
                print("\nQuery results:")
                for row in results:
                    print(row)
                cur.close()
                conn.close()
                return results
            else:
                # For INSERT, UPDATE, DELETE, etc., commit and return None
                conn.commit()
                cur.close()
                conn.close()
                print("Query executed successfully (no results returned).")
                return None
        except Exception as e:
            print(f"Error executing SQL query: {e}")
            return None

import os
import psycopg2
import pandas as pd
import re
import unicodedata
from google.colab import userdata

class DataSync:
    def __init__(self):
        self.db_config = DatabaseConfig()
        self.supabase_dir = "/content/planilhas/supabase"
        self.db_utils = DBUtils()

    def create_table_from_excel(self, table_name: str):
        """
        Reads column names and infers data types from an Excel file to generate a CREATE TABLE script for PostgreSQL and executes it.
        """
        filepath = os.path.join(self.supabase_dir, table_name + ".xlsx")
        print(f"Processing {filepath} for table creation...")
        try:
            df = pd.read_excel(filepath)
            column_info = {}

            # Infer data types
            for col in df.columns:
                # Simple type mapping (can be expanded)
                if df[col].dtype == 'object':
                    column_info[col] = 'TEXT'
                elif df[col].dtype in ['int64', 'int32']:
                    column_info[col] = 'INTEGER'
                elif df[col].dtype in ['float64', 'float32']:
                    column_info[col] = 'REAL'
                elif df[col].dtype == 'datetime64[ns]':
                    column_info[col] = 'TIMESTAMP'
                else:
                    column_info[col] = 'TEXT' # Default to TEXT for unhandled types

            print(f"Inferred column types for '{table_name}': {column_info}")

            # Generate PostgreSQL CREATE TABLE script
            sql_script = f"CREATE TABLE public.{table_name.lower()} (\n"
            for col, col_type in column_info.items():
                # Sanitize column names for SQL (lowercase, replace spaces/special chars with underscore)
                sanitized_col = "".join([c if c.isalnum() or c == '_' else '_' for c in col.lower()]).strip('_')
                sanitized_col = re.sub(r'_{2,}', '_', sanitized_col) # Replace multiple underscores with single

                sql_script += f"  {sanitized_col} {col_type},\n"

            # Add a primary key
            sql_script += "  id SERIAL PRIMARY KEY\n" # Example primary key


            sql_script = sql_script.rstrip(',\n') + "\n);"
            print("\nGenerated SQL Script:")
            print(sql_script)

            # Execute the SQL script against Supabase
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()
            cur.execute(sql_script)
            conn.commit() # Commit the transaction to make the table creation permanent
            cur.close()
            conn.close()
            print(f"Table '{table_name.lower()}' created successfully in Supabase.")

        except FileNotFoundError:
            print(f"Error: File not found at {filepath}")
        except Exception as e:
            print(f"Error processing {filepath} for table creation: {e}")

    def insert_data_from_excel(self, table_name: str):
        """
        Reads data from an Excel file and inserts it into the corresponding PostgreSQL table.
        Assumes column names in Excel match table columns (case-insensitive, sanitized).
        """
        filepath = os.path.join(self.supabase_dir, table_name.upper() + ".xlsx")
        print(f"Processing {filepath} for data insertion into table '{table_name.lower()}'...")
        try:
            df = pd.read_excel(filepath)
            table_name_lower = table_name.lower()

            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()

            # Sanitize column names from DataFrame to match potential database column names
            sanitized_df_columns = [
                re.sub(r'_{2,}', '_', "".join([c if c.isalnum() or c == '_' else '_' for c in col.lower()]).strip('_'))
                for col in df.columns
            ]

            # Generate INSERT statements
            for index, row in df.iterrows():
                # Map values to sanitized column names
                row_data = {sanitized_df_columns[i]: row[col] for i, col in enumerate(df.columns)}

                # Construct the INSERT statement
                columns = ', '.join(row_data.keys())

                # Format values, specifically handling datetime objects for SQL
                formatted_values = []
                for value in row_data.values():
                    if pd.isna(value):
                        formatted_values.append('NULL')
                    elif isinstance(value, pd.Timestamp):
                        # Format datetime objects as ISO 8601 strings
                        formatted_values.append(f"'{value.isoformat()}'")
                    elif isinstance(value, str):
                        # Escape single quotes in strings for SQL
                        formatted_values.append(f"'{value.replace("'", "''")}'")
                    else:
                        formatted_values.append(str(value))

                values = ', '.join(formatted_values)

                sql_insert = f"INSERT INTO public.{table_name_lower} ({columns}) VALUES ({values});"
                #print(f"Executing INSERT: {sql_insert}") # Uncomment for debugging

                try:
                    cur.execute(sql_insert)
                except Exception as e:
                    print(f"Error executing insert for row {index}: {e}\nSQL: {sql_insert}")
                    conn.rollback() # Rollback the transaction on error
                    continue # Continue to the next row

            conn.commit() # Commit all successful insertions
            cur.close()
            conn.close()
            print(f"Data insertion into table '{table_name_lower}' completed.")

        except FileNotFoundError:
            print(f"Error: File not found at {filepath}")
        except Exception as e:
            print(f"Error processing {filepath} for data insertion: {e}")


    def execute(self):
        print("\n< ============================================= >")
        print("Executando fluxo de trabalho DataSync...")

        if os.path.exists(self.supabase_dir):
            excel_files = [f for f in os.listdir(self.supabase_dir) if f.endswith('.xlsx') or f.endswith('.xls')]

            # Extract table names from filenames (remove extension and make lowercase for checking)
            table_names = [os.path.splitext(f)[0] for f in excel_files]

            # Check if each table exists, and if so, drop it before creation
            for tabela in table_names:
                if self.db_utils.table_exists(tabela):
                    print(f"Tabela '{tabela}' existe. Dropando...")
                    self.db_utils.drop_table(tabela)
                    # After dropping, create the table again
                    print(f"Gerando script de criação para '{tabela}'...")
                    self.create_table_from_excel(tabela)
                else:
                    print(f"Tabela '{tabela}' não encontrada. Gerando script de criação...")
                    self.create_table_from_excel(tabela)


            # Now, insert data into the tables that exist (either pre-existing or newly created)
            for tabela in table_names:
                 # You might want to add a check here if insertion is needed (e.g., table is empty)
                 self.insert_data_from_excel(tabela)

        else:
            print(f"Diretório '{self.supabase_dir}' não encontrado.")

        print("Fluxo de trabalho DataSync finalizado.")
        print("< ============================================= >")

import pandas as pd
import os
import re
import unicodedata

class ColumnNameCleaner:
    def __init__(self):
        self.input_folder_path = "/content/planilhas"
        self.output_folder_path = "/content/planilhas/supabase"

    def load_data(self, filepath, filename): # Adicionado filename como parâmetro
        """Carrega um arquivo Excel em um DataFrame pandas, tratando cabeçalhos específicos."""
        try:
            # Para BASE_DIAS_UTEIS.xlsx, use a segunda linha (índice 1) como cabeçalho
            if filename == 'BASE_DIAS_UTEIS.xlsx':
                df = pd.read_excel(filepath, engine='openpyxl', header=1)
                print(f"Carregando {filename} com cabeçalho na linha 2.")
            else:
                # Para outras planilhas, use o cabeçalho padrão (primeira linha)
                df = pd.read_excel(filepath, engine='openpyxl')
                print(f"Carregando {filename} com cabeçalho padrão.")
            return df
        except Exception as e:
             try:
                 # Tenta xlrd se openpyxl falhar, mantendo a lógica de cabeçalho
                 if filename == 'BASE_DIAS_UTEIS.xlsx':
                     df = pd.read_excel(filepath, engine='xlrd', header=1)
                     print(f"Carregando {filename} com cabeçalho na linha 2 (usando xlrd).")
                 else:
                     df = pd.read_excel(filepath, engine='xlrd')
                     print(f"Carregando {filename} com cabeçalho padrão (usando xlrd).")
                 return df
             except Exception as e_read:
                print(f"Erro ao carregar {filepath}: {e_read}. Pulando este arquivo.")
                return None


    def clean_column_name(self, col_name):
        """Limpa um nome de coluna removendo caracteres especiais e ajustando espaços."""
        # Verifica se o nome da coluna contém "UNNAMED" (case-insensitive)
        if "UNNAMED" in col_name.upper():
            return "UNNAMED" # Retorna "UNNAMED" se a condição for verdadeira

        # Remove espaços em branco no início e no final
        cleaned_name = col_name.strip()
        # Remove acentos e caracteres especiais (mantém letras, números, underscores)
        # Normaliza para decompor caracteres acentuados, remove combinadores, mantém alfanuméricos e underscores
        nfkd_form = unicodedata.normalize('NFKD', cleaned_name)
        cleaned_name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
        # Substitui espaços em branco por underscores
        cleaned_name = re.sub(r'\s+', '_', cleaned_name)
        # Remove quaisquer caracteres restantes que não sejam letras, números ou underscores
        cleaned_name = re.sub(r'[^\w_]', '', cleaned_name)
        # Converte para maiúsculas (opcional, mas comum para nomes de colunas de DB)
        cleaned_name = cleaned_name.upper()
        # Remove underscores múltiplos que podem ter sido criados por substituições
        cleaned_name = re.sub(r'_{2,}', '_', cleaned_name)
         # Remove underscore inicial ou final se houver
        cleaned_name = cleaned_name.strip('_')


        return cleaned_name


    def process_spreadsheet(self, filename):
        """Processa uma única planilha: limpa nomes de colunas e salva."""
        input_filepath = os.path.join(self.input_folder_path, filename)
        output_filepath = os.path.join(self.output_folder_path, filename)

        if not os.path.exists(input_filepath):
            print(f"Aviso: Arquivo não encontrado: {input_filepath}. Pulando.")
            return

        print(f"Processando planilha: {filename}")
        # Passa o nome do arquivo para load_data
        df = self.load_data(input_filepath, filename)

        if df is not None:
            original_columns = df.columns.tolist()
            cleaned_columns = [self.clean_column_name(col) for col in original_columns]

            df.columns = cleaned_columns

            # Specific renaming for EXTERIOR.xlsx
            if filename == 'EXTERIOR.xlsx':
                if 'CADASTRO' in df.columns:
                    df.rename(columns={'CADASTRO': 'MATRICULA'}, inplace=True)
                    print("Renomeando coluna 'CADASTRO' para 'MATRICULA' em EXTERIOR.xlsx")

            # Specific renaming for BASE_DIAS_UTEIS.xlsx
            if filename == 'BASE_DIAS_UTEIS.xlsx':
                if 'SINDICADO' in df.columns:
                    df.rename(columns={'SINDICADO': 'SINDICATO'}, inplace=True)
                    print("Renomeando coluna 'SINDICADO' para 'SINDICATO' em BASE_DIAS_UTEIS.xlsx")


            # Crie o diretório de saída se não existir
            os.makedirs(self.output_folder_path, exist_ok=True)

            # Salve o DataFrame modificado
            try:
                df.to_excel(output_filepath, index=False)
                print(f"Planilha processada e salva em: {output_filepath}")
            except Exception as e:
                print(f"Erro ao salvar a planilha {filename}: {e}")

    def execute(self):
        """Executa o processo de limpeza de nomes de colunas para planilhas na pasta de entrada."""
        print("\n< ============================================= >")
        print("Executando Column Name Cleaner workflow...")

        if not os.path.exists(self.input_folder_path):
             print(f"Erro: Pasta de entrada não encontrada em {self.input_folder_path}. Não é possível processar planilhas.")
             return


        # Lista de arquivos na pasta de entrada
        all_files_in_folder = os.listdir(self.input_folder_path)

        # Filtrar para incluir apenas arquivos Excel (.xlsx e .xls) e excluir o arquivo ignorado
        ignored_filename = 'VR_MENSAL_05.2025.xlsx'
        filenames_list = [
            f for f in all_files_in_folder
            if os.path.isfile(os.path.join(self.input_folder_path, f)) # Garante que é um arquivo
            and (f.endswith('.xlsx') or f.endswith('.xls')) # Filtra por extensão Excel
            and f != ignored_filename # Exclui o arquivo ignorado
        ]


        if not filenames_list:
            print(f"Nenhuma planilha Excel encontrada na pasta '{self.input_folder_path}' para processar (excluindo '{ignored_filename}').")
            return

        for filename in filenames_list:
            self.process_spreadsheet(filename)

        print("Column Name Cleaner workflow finalizado.")
        print("< ============================================= >")

import os
import re
import unicodedata
import gdown
import zipfile

class ZipDownloader:
    def __init__(self):
        self.config = ZipDownloaderConfig()

    def sanitize_zip_file(self):
        if os.path.exists(self.config.get_zip_output_path()):
            os.remove(self.config.get_zip_output_path())
            print(f"O arquivo '{self.config.get_zip_output_path()}' foi removido.")
        else:
            print(f"O arquivo '{self.config.get_zip_output_path()}' não existe.")

    def download_zip_file(self):
        print("Downloading...")
        gdown.download(self.config.get_zip_url(), self.config.get_zip_output_path(), quiet=False)
        print(f"'{self.config.get_zip_output_path()}'")

    def sanitize_extract_folder(self):
        if os.path.exists(self.config.get_extract_folder_path()):
            !rm -r {self.config.get_extract_folder_path()}
            print(f"A pasta '{self.config.get_extract_folder_path()}' foi removida.")
        else:
            print(f"A pasta '{self.config.get_extract_folder_path()}' não existe.")

    def extract_zip_file(self):
        print("Extraindo...")
        with zipfile.ZipFile(self.config.get_zip_output_path(), 'r') as zip_ref:
            zip_ref.extractall(self.config.get_extract_folder_path())
        print(f"Arquivo:  {self.config.get_zip_output_path()}")
        for file in os.listdir(self.config.get_extract_folder_path()):
            print(f"           : {os.path.join(self.config.get_extract_folder_path(), file)}")

    def remove_accents_and_spaces(self, filename):
        name, ext = os.path.splitext(filename)
        nfkd_form = unicodedata.normalize('NFKD', name)
        cleaned_name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
        cleaned_name = re.sub(r'[^\w.-]', '_', cleaned_name)
        cleaned_name = re.sub(r'_{2,}', '_', cleaned_name)
        cleaned_name = cleaned_name.upper()
        cleaned_ext = ext.lower()
        return cleaned_name + cleaned_ext

    def adjust_sheet_names(self):
        print("Ajustando nomes das planilhas...")
        for filename in os.listdir(self.config.get_extract_folder_path()):
            old_filepath = os.path.join(self.config.get_extract_folder_path(), filename)
            if os.path.isfile(old_filepath):
                new_filename = self.remove_accents_and_spaces(filename)
                new_filepath = os.path.join(self.config.get_extract_folder_path(), new_filename)
                if old_filepath != new_filepath:
                    try:
                        os.rename(old_filepath, new_filepath)
                        print(f"Renomeado: '{filename}' para '{new_filename}'")
                    except Exception as e:
                        print(f"Erro ao renomear '{filename}': {e}")

    def execute(self):
        print("\n< ============================================= >")
        print("Executando ZipDownloader...")
        self.sanitize_zip_file()
        self.download_zip_file()
        self.sanitize_extract_folder()
        self.extract_zip_file()
        self.adjust_sheet_names()
        print("ZipDownloader Finalizado.")
        print("< ============================================= >")

class Infrastructure:

    def __init__(self, config=None):
        # Default configuration: run all processes
        self.config = {
            'run_zip_downloader': True,
            'run_column_name_cleaner': True,
            'run_data_sync': True,
            'run_data_consolidator': True
        }
        # Update configuration if provided
        if config:
            self.config.update(config)

    def process_zip_downloader(self):
        self.zip_downloader = ZipDownloader()
        self.zip_downloader.execute()

    def process_data_column_name_cleaner(self):
        self.data_column_name_cleaner = ColumnNameCleaner()
        self.data_column_name_cleaner.execute()

    def process_data_sync(self):
        self.data_sync = DataSync()
        self.data_sync.execute()

    def process_data_consolidator(self):
        self.data_consolidator = DataConsolidator()
        self.data_consolidator.execute()

    def run(self):
        if self.config.get('run_zip_downloader'):
            self.process_zip_downloader()
        if self.config.get('run_column_name_cleaner'):
            self.process_data_column_name_cleaner()
        if self.config.get('run_data_sync'):
            self.process_data_sync()
        if self.config.get('run_data_consolidator'):
            self.process_data_consolidator()

import os
import re
import unicodedata
import gdown
import zipfile
import pandas as pd
import psycopg2
from google.colab import userdata

class DataConsolidator:
    def __init__(self):
        self.db_config = DatabaseConfig()
        self.db_utils = DBUtils()

    def generate_consolidated_create_script(self, table_list: list, consolidate_table: str):
        """
        Generates a CREATE TABLE script for a new table based on the columns and data types
        of a list of existing tables in the database, ignoring the 'id' column.
        """
        table_names_string = ','.join(table_list)
        sql_query = f"""
        WITH lista_tabelas AS (
          SELECT unnest(string_to_array('{table_names_string}', ',')) AS table_name
        )
        SELECT 'CREATE TABLE {consolidate_table} (' ||
               string_agg(column_def, ', ') ||
               ');' AS create_table_stmt
        FROM (
          SELECT DISTINCT column_name || ' ' || data_type AS column_def
          FROM information_schema.columns
          WHERE table_schema = 'public'
            AND table_name IN (SELECT table_name FROM lista_tabelas)
            AND column_name <> 'id' -- Exclude the 'id' column
            AND column_name <> 'cargo' -- Exclude the 'cargo' column
        ) sub;
        """
        print("\nGenerated SQL Query to get CREATE TABLE script:")
        print(sql_query)

        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()
            cur.execute(sql_query)
            create_script = cur.fetchone()[0]
            cur.close()
            conn.close()
            print("\nGenerated CREATE TABLE script:")
            print(create_script)
            return create_script

        except Exception as e:
            print(f"Error generating CREATE TABLE script: {e}")
            return None

    def execute_create_script(self, create_script: str):
        print("\nExecutando CREATE TABLE script...")
        print(create_script)
        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()
            cur.execute(create_script)
            conn.commit()
            cur.close()
            conn.close()
            print("CREATE TABLE script executado com sucesso.")
        except Exception as e:
            print(f"Erro na execucao CREATE TABLE script: {e}")

    def create_table_vr(self):
        exec_sql ="""
                  CREATE TABLE public.vr_mensal (
                      matricula INTEGER,
                      admissao DATE,
                      sindicato_colaborador TEXT,
                      competencia TEXT,
                      dias NUMERIC,
                      valor_diario_vr NUMERIC(12, 2),
                      total NUMERIC(12, 2),
                      custo_empresa NUMERIC(12, 2),
                      desconto_profissional NUMERIC(12, 2),
                      obs_geral TEXT
                  );
                  """
        self.db_utils.execute_query(exec_sql)

    def execute(self):
        print("\n< ============================================= >")
        print("Executando fluxo de trabalho DataConsolidator...")

        tables_to_consolidate = ['ativos', 'ferias', 'desligados', 'admissao_abril', 'base_dias_uteis', 'base_sindicato_x_valor']

        if self.db_utils.table_exists('consolidado'):
            print(f"Tabela consolidado existe. Dropando...")
            self.db_utils.drop_table('consolidado')
            # After dropping, create the table again
            print(f"Gerando script de criação para 'consolidado'...")
            create_script = self.generate_consolidated_create_script(tables_to_consolidate, 'consolidado')
        else:
            print(f"Tabela 'consolidado' não encontrada. Gerando script de criação...")
            create_script = self.generate_consolidated_create_script(tables_to_consolidate, 'consolidado')

        if self.db_utils.table_exists('vr_mensal'):
            print(f"Tabela vr_mensal existe. Dropando...")
            self.db_utils.drop_table('vr_mensal')
            self.create_table_vr()
        else:
            print(f"Tabela vr_mensal não encontrada.")
            print(f"Tabela vr_mensal não encontrada. Gerando script de criação...")
            self.create_table_vr()

        if create_script:
             print("CREATE TABLE script gerado. Executando...")
             self.execute_create_script(create_script)

        exec_sql = "CREATE UNIQUE INDEX IF NOT EXISTS idx_consolidado_matricula ON public.consolidado(matricula);"
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  INSERT INTO public.consolidado (
                    matricula, admissao, unnamed
                  )
                  SELECT
                    matricula, admissao, unnamed
                  FROM public.admissao_abril
                  ON CONFLICT (matricula) DO UPDATE SET
                    admissao = EXCLUDED.admissao,
                    unnamed = EXCLUDED.unnamed;
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  UPDATE public.consolidado c
                  SET titulo_do_cargo = a.cargo
                  FROM public.admissao_abril a
                  WHERE c.matricula = a.matricula
                    AND (c.titulo_do_cargo IS NULL OR c.titulo_do_cargo = '');
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  INSERT INTO public.consolidado (
                    matricula, empresa, titulo_do_cargo, desc_situacao, sindicato
                  )
                  SELECT
                    matricula, empresa, titulo_do_cargo, desc_situacao, sindicato
                  FROM public.ativos
                  ON CONFLICT (matricula) DO UPDATE SET
                    empresa = EXCLUDED.empresa,
                    titulo_do_cargo = EXCLUDED.titulo_do_cargo,
                    desc_situacao = EXCLUDED.desc_situacao,
                    sindicato = EXCLUDED.sindicato;
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  INSERT INTO public.consolidado (
                    matricula, data_demissao, comunicado_de_desligamento
                  )
                  SELECT
                    matricula, data_demissao, comunicado_de_desligamento
                  FROM public.desligados
                  ON CONFLICT (matricula) DO UPDATE SET
                    data_demissao = EXCLUDED.data_demissao,
                    comunicado_de_desligamento = EXCLUDED.comunicado_de_desligamento;
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  UPDATE public.consolidado c
                  SET desc_situacao = b.desc_situacao,
                      dias_de_ferias = b.dias_de_ferias
                  FROM public.ferias b
                  WHERE b.matricula = c.matricula;
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  UPDATE public.consolidado c
                  SET dias_uteis = b.dias_uteis
                  FROM public.base_dias_uteis b
                  WHERE
                    substring(c.sindicato from '^[^ -]+') = substring(b.sindicato from '^[^ -]+');
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  UPDATE public.base_sindicato_x_valor
                  SET estado = CASE
                    WHEN estado ILIKE '%Acre%' THEN 'AC'
                    WHEN estado ILIKE '%Alagoas%' THEN 'AL'
                    WHEN estado ILIKE '%Amapá%' THEN 'AP'
                    WHEN estado ILIKE '%Amazonas%' THEN 'AM'
                    WHEN estado ILIKE '%Bahia%' THEN 'BA'
                    WHEN estado ILIKE '%Ceará%' THEN 'CE'
                    WHEN estado ILIKE '%Distrito Federal%' THEN 'DF'
                    WHEN estado ILIKE '%Espírito Santo%' THEN 'ES'
                    WHEN estado ILIKE '%Goiás%' THEN 'GO'
                    WHEN estado ILIKE '%Maranhão%' THEN 'MA'
                    WHEN estado ILIKE '%Mato Grosso%' THEN 'MT'
                    WHEN estado ILIKE '%Mato Grosso do Sul%' THEN 'MS'
                    WHEN estado ILIKE '%Minas Gerais%' THEN 'MG'
                    WHEN estado ILIKE '%Pará%' THEN 'PA'
                    WHEN estado ILIKE '%Paraíba%' THEN 'PB'
                    WHEN estado ILIKE '%Paraná%' THEN 'PR'
                    WHEN estado ILIKE '%Pernambuco%' THEN 'PE'
                    WHEN estado ILIKE '%Piauí%' THEN 'PI'
                    WHEN estado ILIKE '%Rio de Janeiro%' THEN 'RJ'
                    WHEN estado ILIKE '%Rio Grande do Norte%' THEN 'RN'
                    WHEN estado ILIKE '%Rio Grande do Sul%' THEN 'RS'
                    WHEN estado ILIKE '%Rondônia%' THEN 'RO'
                    WHEN estado ILIKE '%Roraima%' THEN 'RR'
                    WHEN estado ILIKE '%Santa Catarina%' THEN 'SC'
                    WHEN estado ILIKE '%São Paulo%' THEN 'SP'
                    WHEN estado ILIKE '%Sergipe%' THEN 'SE'
                    WHEN estado ILIKE '%Tocantins%' THEN 'TO'
                    ELSE estado -- manter valor original se não casar com nenhum
                  END;
                  """
        self.db_utils.execute_query(exec_sql)

        exec_sql ="""
                  UPDATE public.consolidado AS c
                  SET
                      estado = b.estado,
                      valor  = b.valor
                  FROM public.base_sindicato_x_valor AS b
                  WHERE
                      -- take the second word of c.sindicato (the state sigla)
                      upper(split_part(c.sindicato, ' ', 2)) = upper(b.estado);
                  """
        self.db_utils.execute_query(exec_sql)

        print("Fluxo de trabalho DataConsolidator finalizado.")
        print("< ============================================= >")

import pandas as pd
import os
from google.colab import userdata
import psycopg2
from datetime import datetime
import shutil
import re
import unicodedata
import openpyxl
import google.generativeai as genai

class DataExport:

    def __init__(self,config=None):
        self.db_config = DatabaseConfig()
        self.db_utils = DBUtils()
        self.template_path = "/content/planilhas/VR_MENSAL_05.2025.xlsx"
        self.output_dir = "/content/resultado"
        os.makedirs(self.output_dir, exist_ok=True)
        # Default configuration: run all processes
        self.config = {
            'run_export_vr_mensal_to_excel': True
        }
        # Update configuration if provided
        if config:
            self.config.update(config)

    def export_vr_mensal_to_excel(self):
        output_filename = f"VR MENSAL 05.2025.xlsx"
        output_filepath = os.path.join(self.output_dir, output_filename)

        print(f"\nExporting data from 'vr_mensal' to '{output_filepath}'...")

        # 1. Copy the template file
        try:
            shutil.copy(self.template_path, output_filepath)
            print(f"Template '{self.template_path}' copied to '{output_filepath}'.")
        except FileNotFoundError:
            print(f"Error: Template file not found at '{self.template_path}'. Cannot proceed with export.")
            return
        except Exception as e:
            print(f"Error copying template file: {e}")
            return

        # 2. Read data from the vr_mensal table
        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            sql_query = "SELECT * FROM public.vr_mensal;"
            df_vr_mensal = pd.read_sql_query(sql_query, conn)
            conn.close()
            print(f"Successfully read {len(df_vr_mensal)} rows from 'vr_mensal'.")

        except Exception as e:
            print(f"Error reading data from 'vr_mensal' table: {e}")
            return

        # 3. Read the template Excel file to get column names and structure
        try:
            # Use openpyxl engine for potentially better compatibility
            with pd.ExcelFile(output_filepath, engine='openpyxl') as xls:
                sheet_names = xls.sheet_names
                if not sheet_names:
                    print(f"Error: No sheets found in the template file '{output_filepath}'.")
                    return
                # Assuming the data should be written to the first sheet
                template_sheet_name = sheet_names[0]
                print(f"Template sheet name: '{template_sheet_name}'")

                # Read the template using the second row (index 1) as the header
                df_template = pd.read_excel(xls, sheet_name=template_sheet_name, header=1) # Use header=1 for the second row
                print("Template columns after reading with identified header:")
                print(df_template.columns.tolist())

        except Exception as e:
            print(f"Error reading the template Excel file '{output_filepath}': {e}")
            return


        # 4. Map database columns to Excel columns and prepare data for writing
        column_mapping = {
            'matricula': 'Matricula',
            'admissao': 'Admissão',
            'sindicato_colaborador': 'Sindicato do Colaborador',
            'competencia': 'Competência',
            'dias': 'Dias',
            'valor_diario_vr': 'VALOR DIÁRIO VR',
            'total': 'TOTAL',
            'custo_empresa': 'Custo empresa',
            'desconto_profissional': 'Desconto profissional',
            'obs_geral': 'OBS GERAL'
        }

        # Create a new DataFrame with columns ordered and named according to the template
        df_ordered = pd.DataFrame()
        # Ensure that the template column names are handled case-insensitively during mapping
        template_columns_lower_to_exact = {col.lower(): col for col in df_template.columns.tolist()}

        for db_col, excel_col in column_mapping.items():
             # Find the exact column name in the template DataFrame (case-insensitive match)
             excel_col_lower = excel_col.lower()
             if excel_col_lower in template_columns_lower_to_exact:
                 template_exact_col_name = template_columns_lower_to_exact[excel_col_lower]
                 df_ordered[template_exact_col_name] = df_vr_mensal[db_col]
             else:
                 print(f"Warning: Column '{excel_col}' (case-insensitive) not found in the template file. Skipping.")


        print("\nDataFrame to be written (df_ordered) head:")
        display(df_ordered.head())


        # 5. Write the data to the copied Excel file
        try:
            # Use ExcelWriter to write to a specific sheet without overwriting the entire file
            with pd.ExcelWriter(output_filepath, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
                 # Find the starting row to write data (one row below the header, which is row 1)
                 startrow = 2
                 df_ordered.to_excel(writer, sheet_name=template_sheet_name, index=False, header=False, startrow=startrow)

            print(f"Data successfully inserted into '{output_filepath}'.")

        except Exception as e:
            print(f"Error writing data to Excel file '{output_filepath}': {e}")

        # 6. Calculate sum of column G (index 6) from row 3 (index 2) onwards and write to G1 (index 0, index 6)
        try:
            workbook = openpyxl.load_workbook(output_filepath)
            sheet = workbook[template_sheet_name]

            # Identify the column letter for 'TOTAL'
            total_col_letter = None
            for col_idx, col in enumerate(df_template.columns):
                if col.lower() == 'total': # Case-insensitive match for 'TOTAL'
                     total_col_letter = openpyxl.utils.get_column_letter(col_idx + 1) # Convert 0-based index to 1-based letter
                     break

            if total_col_letter:
                 # Calculate the sum of the 'TOTAL' column starting from the row after the header
                 # The header is at row 2 (index 1), data starts at row 3 (index 2)
                 start_row_for_sum = 3
                 end_row_for_sum = start_row_for_sum + len(df_ordered) -1 # Sum up to the last row of inserted data

                 # Construct the Excel formula for the sum
                 sum_formula = f'=SUM({total_col_letter}{start_row_for_sum}:{total_col_letter}{end_row_for_sum})'

                 # Write the formula to the cell in the first row (index 1) and the 'TOTAL' column
                 # openpyxl is 1-based for rows and columns
                 sheet[f'{total_col_letter}1'] = sum_formula
                 print(f"Formula '{sum_formula}' written to cell {total_col_letter}1.")

                 workbook.save(output_filepath)
                 print(f"Sum calculated and written to '{output_filepath}'.")
            else:
                print("Warning: 'TOTAL' column not found in the template. Cannot calculate sum.")

        except Exception as e:
            print(f"Error calculating and writing sum to Excel file: {e}")


    def run(self):
        if self.config.get('run_export_vr_mensal_to_excel'):
          self.export_vr_mensal_to_excel()

import pandas as pd
import os
from google.colab import userdata
import psycopg2
from datetime import datetime
import shutil
import re
import unicodedata
import openpyxl
import google.generativeai as genai

class AutomatedBenefitCalculation:
    def __init__(self, config=None):
        # Default configuration: run all processes
        self.config = {
            'run_calculate_benefit': True
        }
        # Update configuration if provided
        if config:
            self.config.update(config)
    def calculate_benefit(self):
      print("calculate_benefit")

    def run(self):
      if self.config.get('run_analyzing_agent'):
        print("Run")

import pandas as pd
from google.colab import userdata
import psycopg2
import json

class ExclusionAgent:

    def __init__(self, config=None):
        self.db_config = DatabaseConfig()
        self.db_utils = DBUtils()
        self.gemini_agent = GeminiAgent()
        self.excluded_matriculas = []
        self.data_frame_consolidado = DataFrameConsolidado()
        # Default configuration: run all processes
        self.config = {
            'run_analyzing_agent': True,
            'run_delete_consolidado': True
        }
        # Update configuration if provided
        if config:
            self.config.update(config)


    def get_dataframe(self):
        print("\nLendo dados da tabela 'consolidado'...")
        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            sql_query = "SELECT * FROM public.consolidado;"
            df_consolidado = pd.read_sql_query(sql_query, conn)
            conn.close()
            print(f"Lidas {len(df_consolidado)} linhas da tabela 'consolidado'.")
            return df_consolidado

        except Exception as e:
            print(f"Erro ao ler dados da tabela 'consolidado': {e}")
            return None

    def delete_consolidado(self):
        if not self.excluded_matriculas:
            print("Nenhuma matrícula para excluir.")
            return

        print(f"\nExcluindo {len(self.excluded_matriculas)} registros da tabela 'consolidado'...")
        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            cur = conn.cursor()

            for matricula in self.excluded_matriculas:
                try:
                    exec_sql = f"DELETE FROM public.consolidado WHERE matricula = {matricula};"
                    cur.execute(exec_sql)
                    print(f"Matrícula excluída: {matricula}")
                except Exception as e:
                    print(f"Erro ao excluir matrícula {matricula}: {e}")
                    conn.rollback() # Reverte a transação em caso de erro
                    continue # Continua para a próxima matrícula

            conn.commit() # Confirma todas as exclusões bem-sucedidas
            cur.close()
            conn.close()
            print("Processo de exclusão concluído.")

        except Exception as e:
            print(f"Erro ao conectar ao banco de dados para exclusão: {e}")

    def analyzing_agent(self):
        df = self.data_frame_consolidado.get_dataframe()
        if df is None:
            print("DataFrame é None. Saindo.")
            return

        chunk_size = 250
        num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size > 0 else 0)

        for i in range(num_chunks):
            start_index = i * chunk_size
            end_index = min((i + 1) * chunk_size, len(df))
            df_chunk = df.iloc[start_index:end_index]

            print(f"\nProcessando chunk {i + 1}/{num_chunks} (linhas {start_index + 1} a {end_index})...")

            prompt = f"""
              Abaixo está um trecho de uma planilha de funcionários em formato CSV:
              { df_chunk.to_csv(index=False) }
              Regras de exclusão:
              - Exclua funcionários cuja coluna 'titulo_do_cargo' contenha: diretor, estagiário(a) ou aprendiz (qualquer variação ou acentuação).
              - Exclua funcionários cuja coluna 'desc_situacao' indique qualquer tipo de afastamento, como: licença maternidade, licença médica, afastado(a) ou dispensado(a).

              Retorn uma lista de todas as matrículas (campo 'matricula') que foram excluídas conforme as regras, separadas por vírgula, sem detalhamento extra
              Retorne apenas um objeto JSON e nada alem disso,no seguinte formato:
              "matriculas": []
            """
            result = self.gemini_agent.generate(prompt)
            #print("result chunk:\n", result)

            # Processar o resultado da string para extrair matrículas
            try:
                # Remove potenciais delimitadores de bloco de código markdown e a palavra 'json'
                cleaned_result = result.strip().replace("```json", "").replace("```", "").strip()
                data = json.loads(cleaned_result)
                if "matriculas" in data and isinstance(data["matriculas"], list):
                    self.excluded_matriculas.extend(data["matriculas"])
                    print(f"Extraídas {len(data['matriculas'])} matrículas do chunk {i+1}.")
                else:
                    print(f"Aviso: Chave 'matriculas' não encontrada ou não é uma lista na resposta JSON para o chunk {i+1}.")
            except json.JSONDecodeError as e:
                print(f"Erro ao decodificar JSON da resposta do Gemini para o chunk {i+1}: {e}")
                print(f"String problemática: {cleaned_result}")
            except Exception as e:
                print(f"Ocorreu um erro inesperado ao processar a resposta do Gemini para o chunk {i+1}: {e}")


        print("\nTodos os chunks processados.")
        print(f"Total de matrículas excluídas coletadas: {len(self.excluded_matriculas)}")
        # print("Excluded matriculas:", self.excluded_matriculas)

    def run(self):
        if self.config.get('run_analyzing_agent'):
            self.analyzing_agent()
        if self.config.get('run_delete_consolidado'):
             self.delete_consolidado()

import pandas as pd
from google.colab import userdata
import psycopg2
import json

class DataFrameConsolidado:

    def __init__(self):
        self.db_config = DatabaseConfig()
        self.db_utils = DBUtils()
        self.matriculas = []

    def get_dataframe(self):
        print("\nLendo dados da tabela 'consolidado'...")
        try:
            conn = psycopg2.connect(
                host=self.db_config.get_db_host(),
                dbname=self.db_config.get_db_name(),
                user=self.db_config.get_db_user(),
                password=self.db_config.get_db_pass(),
                port=self.db_config.get_db_port()
            )
            sql_query = "SELECT * FROM public.consolidado;"
            df_consolidado = pd.read_sql_query(sql_query, conn)
            conn.close()
            print(f"Lidas {len(df_consolidado)} linhas da tabela 'consolidado'.")
            return df_consolidado

        except Exception as e:
            print(f"Erro ao ler dados da tabela 'consolidado': {e}")
            return None

import pandas as pd
from google.colab import userdata
import psycopg2
import json

class VrMensalSqlAgent:

    def __init__(self, config=None):
        self.db_config = DatabaseConfig()
        self.db_utils = DBUtils()
        self.gemini_agent = GeminiAgent()
        self.sql_insert_vr_mensal = None
        # Default configuration: run all processes
        self.config = {
            'run_analyzing_agent': True,
            'run_insert_vr_mensal': True
        }
        # Update configuration if provided
        if config:
            self.config.update(config)

    def insert_vr_mensal(self):
      if self.sql_insert_vr_mensal is None:
          print("sql_insert_vr_mensal esta vazio. Saindo.")
          return

      print("executando sql_insert_vr_mensal.")
      self.db_utils.execute_query(self.sql_insert_vr_mensal)

    def analyzing_agent(self):

        prompt = f"""
          Table 1: public.consolidado
          Columns:
          admissao (timestamp),
          comunicado_de_desligamento (text),
          data_demissao (timestamp),
          desc_situacao (text),
          dias_de_ferias (integer),
          dias_uteis (integer),
          empresa (integer), estado (text),
          matricula (integer),
          sindicato (text),
          titulo_do_cargo (text),
          unnamed (text),
          valor (real)

          Table 2: public.vr_mensal
          Columns:
          matricula (integer),
          admissao (date),
          sindicato_colaborador (text),
          competencia (text),
          dias (numeric),
          valor_diario_vr (numeric),
          total (numeric),
          custo_empresa (numeric),
          desconto_profissional (numeric),
          obs_geral (text)

          Write an SQL statement that will insert records into public.vr_mensal
          using data from public.consolidado with the following column mappings and calculations:

          matricula in vr_mensal ← matricula in consolidado

          admissao in vr_mensal ← data_demissao in consolidado, cast as date

          sindicato_colaborador in vr_mensal ← sindicato in consolidado

          competencia in vr_mensal must always be '05/2025'

          dias in vr_mensal ← dias_uteis in consolidado

          valor_diario_vr in vr_mensal ← valor in consolidado

          total in vr_mensal ← multiplication of dias by valor_diario_vr

          custo_empresa in vr_mensal ← 80% of total

          desconto_profissional in vr_mensal ← 20% of total

          obs_geral in vr_mensal should be left blank/null

          Please generate the complete SQL code for this operation.
        """
        print("prompt:\n", prompt)
        result = self.gemini_agent.generate(prompt)
        result = result.strip().replace("```sql", "").replace("```", "").strip()
        print("resultado do prompt:\n", result)
        self.sql_insert_vr_mensal = result

    def run(self):
        if self.config.get('run_analyzing_agent'):
            self.analyzing_agent()
        if self.config.get('run_insert_vr_mensal'):
             self.insert_vr_mensal()

import google.generativeai as genai
from google.colab import userdata
import time

class GeminiAgent:
    def __init__(self):
        genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))
        # Usar modelo Gemini gratuito (pode ser configurado via secrets)
        self.model = genai.GenerativeModel(userdata.get('GOOGLE_GENERATIVE_MODEL') or 'gemini-2.5-flash')
        self._call_timestamps = [] # Lista para armazenar os timestamps das chamadas
        # Limite máximo de chamadas (configurável via secrets, default 5)
        self._rate_limit = int(userdata.get('GOOGLE_GENAI_RATE_LIMIT') or '5')
        # Janela de tempo em segundos para o limite de chamadas (configurável via secrets, default 60)
        self._rate_limit_window = float(userdata.get('GOOGLE_GENAI_RATE_LIMIT_WINDOW') or '60')

    def generate(self, prompt: str) -> str:
        current_time = time.time()
        self._call_timestamps.append(current_time)

        # Remover timestamps fora da janela atual
        self._call_timestamps = [ts for ts in self._call_timestamps if current_time - ts <= self._rate_limit_window]

        # Verificar se o limite de taxa foi excedido
        if len(self._call_timestamps) > self._rate_limit:
            # Calcular o tempo de espera até que a janela da chamada mais antiga passe
            time_to_wait = self._call_timestamps[0] + self._rate_limit_window - current_time
            if time_to_wait > 0:
                print(f"Limite de taxa excedido. Aguardando por {time_to_wait:.2f} segundos.")
                time.sleep(time_to_wait)
                # Após aguardar, atualizar o tempo atual e recalcular os timestamps
                current_time = time.time()
                self._call_timestamps = [ts for ts in self._call_timestamps if current_time - ts <= self._rate_limit_window]


        print("Prompt enviado para o Gemini")
        response = self.model.generate_content(prompt)
        return response.text.strip()

# Preparar a infraestrutura com configuração
infra_config = {
  'run_zip_downloader': False,
  'run_column_name_cleaner': False,
  'run_data_sync': False,
  'run_data_consolidator': False
}
infra = Infrastructure(config=infra_config)
infra.run()

# Tratamento de exclusões com configuração
exclusion_config = {
  'run_analyzing_agent': True,
  'run_delete_consolidado': True
}
exclusion = ExclusionAgent(config=exclusion_config)
exclusion.run()

# Agente para gerar sql de calculos na vr_mensal
vr_sql_agent_config = {
  'run_analyzing_agent': True,
  'run_insert_vr_mensal': True
}
vr_sql_agent = VrMensalSqlAgent(config=vr_sql_agent_config)
vr_sql_agent.run()

# Processo para exportar dados para Planilha VR MENSAL 05.2025.xlsx
export_config = {
  'run_export_vr_mensal_to_excel': True
}
export = DataExport(config=exclusion_config)
export.run()